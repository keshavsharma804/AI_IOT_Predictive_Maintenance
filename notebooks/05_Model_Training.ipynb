{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f55d0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.config import get_config\n",
    "from src.features.feature_engineering import FeatureEngineer\n",
    "from src.models.isolation_forest_model import IsolationForestModel\n",
    "from src.models.model_trainer import ModelTrainer\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "\n",
    "# %% Cell 2: Load Features\n",
    "config = get_config()\n",
    "engineer = FeatureEngineer(config)\n",
    "\n",
    "print(\"Loading features...\")\n",
    "features_df = engineer.load_features('machine_001_features.csv')\n",
    "\n",
    "print(f\"\\nDataset Info:\")\n",
    "print(f\"  Shape: {features_df.shape}\")\n",
    "print(f\"  Anomalies: {features_df['is_anomaly'].sum()} ({features_df['is_anomaly'].mean()*100:.2f}%)\")\n",
    "print(f\"  Time range: {features_df['window_start'].min()} to {features_df['window_end'].max()}\")\n",
    "\n",
    "# %% Cell 3: Data Preparation\n",
    "trainer = ModelTrainer(config)\n",
    "\n",
    "print(\"Preparing data...\")\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = trainer.prepare_data(\n",
    "    features_df,\n",
    "    test_size=0.2,\n",
    "    val_size=0.1\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Data prepared:\")\n",
    "print(f\"  Train: {len(X_train)} samples\")\n",
    "print(f\"  Val:   {len(X_val)} samples\")\n",
    "print(f\"  Test:  {len(X_test)} samples\")\n",
    "print(f\"  Features: {X_train.shape[1]}\")\n",
    "\n",
    "# %% Cell 4: Train Model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING ISOLATION FOREST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model = IsolationForestModel(config)\n",
    "model.train(X_train, y_train)\n",
    "\n",
    "print(\"\\n✓ Training complete!\")\n",
    "\n",
    "# %% Cell 5: Evaluate Model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Evaluate on test set\n",
    "metrics = model.evaluate(X_test, y_test)\n",
    "model.print_metrics()\n",
    "\n",
    "# Get predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_scores = model.predict_scores(X_test)\n",
    "\n",
    "# %% Cell 6: Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "           xticklabels=['Normal', 'Anomaly'],\n",
    "           yticklabels=['Normal', 'Anomaly'],\n",
    "           cbar_kws={'label': 'Count'})\n",
    "\n",
    "ax.set_xlabel('Predicted', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Actual', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Confusion Matrix - Isolation Forest', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add percentages\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        percentage = cm[i, j] / cm.sum() * 100\n",
    "        ax.text(j+0.5, i+0.7, f'({percentage:.1f}%)', \n",
    "               ha='center', va='center', fontsize=10, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/confusion_matrix_notebook.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  True Negatives:  {cm[0,0]:,} (Correctly identified normal)\")\n",
    "print(f\"  False Positives: {cm[0,1]:,} (False alarms)\")\n",
    "print(f\"  False Negatives: {cm[1,0]:,} (Missed anomalies)\")\n",
    "print(f\"  True Positives:  {cm[1,1]:,} (Correctly detected anomalies)\")\n",
    "\n",
    "# %% Cell 7: ROC Curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.plot(fpr, tpr, linewidth=2, label=f'Isolation Forest (AUC = {roc_auc:.3f})')\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_title('ROC Curve', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/roc_curve_notebook.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nROC-AUC Score: {roc_auc:.4f}\")\n",
    "print(f\"Interpretation: {'Excellent' if roc_auc > 0.9 else 'Good' if roc_auc > 0.8 else 'Fair'}\")\n",
    "\n",
    "# %% Cell 8: Precision-Recall Curve\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "precision, recall, thresholds_pr = precision_recall_curve(y_test, y_scores)\n",
    "ap = average_precision_score(y_test, y_scores)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.plot(recall, precision, linewidth=2, label=f'Isolation Forest (AP = {ap:.3f})')\n",
    "ax.axhline(y=y_test.mean(), color='r', linestyle='--', linewidth=1, \n",
    "          label=f'Baseline (Random): {y_test.mean():.3f}')\n",
    "\n",
    "ax.set_xlabel('Recall', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Precision', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/pr_curve_notebook.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAverage Precision: {ap:.4f}\")\n",
    "\n",
    "# %% Cell 9: Threshold Optimization\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"THRESHOLD OPTIMIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate F1 scores for different thresholds\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "\n",
    "# Find optimal threshold\n",
    "best_idx = np.argmax(f1_scores)\n",
    "optimal_threshold = thresholds_pr[best_idx]\n",
    "best_precision = precision[best_idx]\n",
    "best_recall = recall[best_idx]\n",
    "best_f1 = f1_scores[best_idx]\n",
    "\n",
    "print(f\"\\nOptimal Threshold: {optimal_threshold:.4f}\")\n",
    "print(f\"  Precision: {best_precision:.4f}\")\n",
    "print(f\"  Recall:    {best_recall:.4f}\")\n",
    "print(f\"  F1-Score:  {best_f1:.4f}\")\n",
    "\n",
    "# Plot threshold vs metrics\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(thresholds_pr, precision[:-1], label='Precision', linewidth=2)\n",
    "ax.plot(thresholds_pr, recall[:-1], label='Recall', linewidth=2)\n",
    "ax.plot(thresholds_pr, f1_scores[:-1], label='F1-Score', linewidth=2)\n",
    "ax.axvline(optimal_threshold, color='red', linestyle='--', linewidth=2, \n",
    "          label=f'Optimal: {optimal_threshold:.4f}')\n",
    "\n",
    "ax.set_xlabel('Threshold', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Threshold Optimization', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/threshold_optimization.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# %% Cell 10: Anomaly Score Distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(y_scores[y_test == 0], bins=50, alpha=0.6, label='Normal', \n",
    "            color='green', density=True, edgecolor='black')\n",
    "axes[0].hist(y_scores[y_test == 1], bins=50, alpha=0.6, label='Anomaly', \n",
    "            color='red', density=True, edgecolor='black')\n",
    "axes[0].set_xlabel('Anomaly Score', fontsize=12)\n",
    "axes[0].set_ylabel('Density', fontsize=12)\n",
    "axes[0].set_title('Anomaly Score Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "data_to_plot = [y_scores[y_test == 0], y_scores[y_test == 1]]\n",
    "bp = axes[1].boxplot(data_to_plot, labels=['Normal', 'Anomaly'], patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor('green')\n",
    "bp['boxes'][1].set_facecolor('red')\n",
    "for box in bp['boxes']:\n",
    "    box.set_alpha(0.6)\n",
    "axes[1].set_ylabel('Anomaly Score', fontsize=12)\n",
    "axes[1].set_title('Anomaly Score by Class', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/anomaly_score_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAnomaly Score Statistics:\")\n",
    "print(f\"  Normal   - Mean: {y_scores[y_test == 0].mean():.4f}, Std: {y_scores[y_test == 0].std():.4f}\")\n",
    "print(f\"  Anomaly  - Mean: {y_scores[y_test == 1].mean():.4f}, Std: {y_scores[y_test == 1].std():.4f}\")\n",
    "print(f\"  Separation: {abs(y_scores[y_test == 1].mean() - y_scores[y_test == 0].mean()):.4f}\")\n",
    "\n",
    "# %% Cell 11: Prediction Examples\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREDICTION EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show some examples\n",
    "num_examples = 10\n",
    "example_indices = np.random.choice(len(X_test), num_examples, replace=False)\n",
    "\n",
    "examples_df = pd.DataFrame({\n",
    "    'Actual': y_test.iloc[example_indices].values,\n",
    "    'Predicted': y_pred[example_indices],\n",
    "    'Anomaly_Score': y_scores[example_indices],\n",
    "    'Correct': (y_test.iloc[example_indices].values == y_pred[example_indices])\n",
    "})\n",
    "\n",
    "examples_df['Actual'] = examples_df['Actual'].map({0: 'Normal', 1: 'Anomaly'})\n",
    "examples_df['Predicted'] = examples_df['Predicted'].map({0: 'Normal', 1: 'Anomaly'})\n",
    "\n",
    "print(\"\\nSample Predictions:\")\n",
    "print(examples_df.to_string(index=False))\n",
    "\n",
    "# %% Cell 12: Feature Importance\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nCalculating feature importance (using subset of test data)...\")\n",
    "importance = model.get_feature_importance(X_test.iloc[:100], n_top=20)\n",
    "\n",
    "print(\"\\nTop 20 Most Important Features:\")\n",
    "print(importance.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "y_pos = np.arange(len(importance))\n",
    "ax.barh(y_pos, importance['importance'], color='skyblue', edgecolor='black')\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels([f.replace('vibration_rms_', 'vib_')[:40] for f in importance['feature']], \n",
    "                   fontsize=9)\n",
    "ax.set_xlabel('Importance Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Top 20 Most Important Features', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# %% Cell 13: Business Impact Analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BUSINESS IMPACT ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get business costs from config\n",
    "cost_downtime = config.get('business.cost_unplanned_downtime', 5000)  # per hour\n",
    "cost_maintenance = config.get('business.cost_planned_maintenance', 500)\n",
    "cost_false_alarm = config.get('business.cost_false_alarm', 100)\n",
    "\n",
    "# Calculate costs\n",
    "tp = metrics['true_positive']\n",
    "fp = metrics['false_positive']\n",
    "fn = metrics['false_negative']\n",
    "tn = metrics['true_negative']\n",
    "\n",
    "# Assuming each anomaly if undetected leads to 4 hours downtime\n",
    "downtime_prevented = tp * 4 * cost_downtime\n",
    "false_alarm_cost = fp * cost_false_alarm\n",
    "missed_failures_cost = fn * 4 * cost_downtime\n",
    "maintenance_cost = tp * cost_maintenance\n",
    "\n",
    "net_benefit = downtime_prevented - false_alarm_cost - maintenance_cost\n",
    "roi = (net_benefit / (false_alarm_cost + maintenance_cost)) * 100 if (false_alarm_cost + maintenance_cost) > 0 else 0\n",
    "\n",
    "print(f\"\\nBusiness Metrics:\")\n",
    "print(f\"  Downtime Prevented:    ${downtime_prevented:,.2f}\")\n",
    "print(f\"  False Alarm Costs:     ${false_alarm_cost:,.2f}\")\n",
    "print(f\"  Missed Failure Costs:  ${missed_failures_cost:,.2f}\")\n",
    "print(f\"  Maintenance Costs:     ${maintenance_cost:,.2f}\")\n",
    "print(f\"  Net Benefit:           ${net_benefit:,.2f}\")\n",
    "print(f\"  ROI:                   {roi:.1f}%\")\n",
    "\n",
    "# %% Cell 14: Save Model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model_path = config.get('paths.models', 'models/saved_models') + '/isolation_forest.pkl'\n",
    "model.save(model_path)\n",
    "print(f\"✓ Model saved to: {model_path}\")\n",
    "\n",
    "# %% Cell 15: Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n✓ Model: Isolation Forest\")\n",
    "print(f\"✓ Training set: {len(X_train):,} samples\")\n",
    "print(f\"✓ Test set: {len(X_test):,} samples\")\n",
    "print(f\"✓ Features: {X_train.shape[1]}\")\n",
    "\n",
    "print(f\"\\n✓ Performance:\")\n",
    "print(f\"  Accuracy:  {metrics['accuracy']:.2%}\")\n",
    "print(f\"  Precision: {metrics['precision']:.2%}\")\n",
    "print(f\"  Recall:    {metrics['recall']:.2%}\")\n",
    "print(f\"  F1-Score:  {metrics['f1_score']:.2%}\")\n",
    "print(f\"  ROC-AUC:   {metrics['roc_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\n✓ Business Impact:\")\n",
    "print(f\"  Net Benefit: ${net_benefit:,.2f}\")\n",
    "print(f\"  ROI: {roi:.1f}%\")\n",
    "\n",
    "print(f\"\\n✓ Model Status: Ready for deployment!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Training complete!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
